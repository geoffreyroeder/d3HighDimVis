\documentclass{sigchi}

% Use this command to override the default ACM copyright statement
% (e.g. for preprints).  Consult the conference website for the
% camera-ready copyright statement.

%% EXAMPLE BEGIN -- HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP -- (July 22, 2013 - Paul Baumann)
% \toappear{Permission to make digital or hard copies of all or part of this work for personal or classroom use is      granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \\
% {\emph{CHI'14}}, April 26--May 1, 2014, Toronto, Canada. \\
% Copyright \copyright~2014 ACM ISBN/14/04...\$15.00. \\
% DOI string from ACM form confirmation}
%% EXAMPLE END -- HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP -- (July 22, 2013 - Paul Baumann)

% Arabic page numbers for submission.  Remove this line to eliminate
% page numbers for the camera ready copy
% \pagenumbering{arabic}

% Load basic packages
\usepackage{balance}  % to better equalize the last page
\usepackage{graphics} % for EPS, load graphicx instead 
\usepackage[T1]{fontenc}
\usepackage{txfonts}
\usepackage{mathptmx}
\usepackage[pdftex]{hyperref}
\usepackage{color}
\usepackage{booktabs}
\usepackage{textcomp}
% Some optional stuff you might like/need.
\usepackage{microtype} % Improved Tracking and Kerning
% \usepackage[all]{hypcap}  % Fixes bug in hyperref caption linking
\usepackage{ccicons}  % Cite your images correctly!
% \usepackage[utf8]{inputenc} % for a UTF8 editor only

% If you want to use todo notes, marginpars etc. during creation of your draft document, you
% have to enable the "chi_draft" option for the document class. To do this, change the very first
% line to: "\documentclass[chi_draft]{sigchi}". You can then place todo notes by using the "\todo{...}"
% command. Make sure to disable the draft option again before submitting your final document.
\usepackage{todonotes}

% Paper metadata (use plain text, for PDF inclusion and later
% re-using, if desired).  Use \emtpyauthor when submitting for review
% so you remain anonymous.
\def\plaintitle{Lifting the Curse of Dimensionality: High-Dimensional Visual Informatics using d3}
\def\plainauthor{Geoffrey G. Roeder}
%\def\emptyauthor{}
\def\plainkeywords{Authors' choice; of terms; separated; by
  semicolons; include commas, within terms only; required.}
\def\plaingeneralterms{Documentation, Standardization}

% llt: Define a global style for URLs, rather that the default one
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{
    \def\UrlFont{\sf}
  }{
    \def\UrlFont{\small\bf\ttfamily}
  }}
\makeatother
\urlstyle{leo}

% To make various LaTeX processors do the right thing with page size.
\def\pprw{8.5in}
\def\pprh{11in}
\special{papersize=\pprw,\pprh}
\setlength{\paperwidth}{\pprw}
\setlength{\paperheight}{\pprh}
\setlength{\pdfpagewidth}{\pprw}
\setlength{\pdfpageheight}{\pprh}

% Make sure hyperref comes last of your loaded packages, to give it a
% fighting chance of not being over-written, since its job is to
% redefine many LaTeX commands.
\definecolor{linkColor}{RGB}{6,125,233}
\hypersetup{%
  pdftitle={\plaintitle},
% Use \plainauthor for final version.
%  pdfauthor={\plainauthor},
  pdfauthor={\plainauthor},
  pdfkeywords={\plainkeywords},
  bookmarksnumbered,
  pdfstartview={FitH},
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=linkColor,
  breaklinks=true,
}

% create a shortcut to typeset table headings
% \newcommand\tabhead[1]{\small\textbf{#1}}

% End of preamble. Here it comes the document.
\begin{document}

\title{\plaintitle}

\numberofauthors{3}
\author{%
  \alignauthor{Geoffrey G. Roeder\\
    \affaddr{University of Toronto}\\
    \email{roeder@cs.toronto.edu}}\\
}

\maketitle

\begin{abstract}
PLACEHOLDER To address this visualization gap between classical statistics and machine learning, my project will produce a new software tool that implements automated dimensionality reduction algorithms from the domain of visual analytics like t-SNE (Hinton \& Van der Maatens 2008), Multi-Dimensional Scaling (Kruskal \& Wish, 1978) and Sammon Mapping (Sammon, 1969) in the context of a D3.js visualization interface. The interface will provide users with multiple reduced dimensionality versions of a dataset that can be clustered using the algorithms DBSCAN density-based clustering, (Ester et al. 1996), k-means centroid-based clustering (Hartigan \& Wong 1979), or mixture-of-Gaussians distribution-based clustering. The D3.js interface is a way for the tool to present the visualizations in an easy-to-modify form. The software provides easy-to-use sliders for adjusting the parameters and viewing boxes that allow a researcher to experiment and iterate quickly over different visualizations 
\end{abstract}

\category{H.5.m.}{Information Interfaces and Presentation
  (e.g. HCI)}{Miscellaneous} \category{See
  \url{http://acm.org/about/class/1998/} for the full list of ACM
  classifiers. This section is required.}{}{}

\keywords{\plainkeywords}

\section{Introduction}

Machine learning in practice follows a pipeline that begins with data cleaning and exploration and ends with a statistical model tailored to answer a particular question. % 
%
Information visualization can help a practitioner with this pipeline by providing valuable visualizations of high-dimensional data to aid in the formation of models that are effective for the task at hand. % 
%
Particular challenges in machine learning make existing tools inadequate. % 
Unlike data collected from carefully designed statistical experiments, the data handled in machine learning is often collected as a by-product of some other business process, such as the scrolling, clicking, and purchase behaviour of customers while browsing a retail website. % 
The features of such datasets are often redundant, and so they exhibit high statistical correlation with each other. % 
%
Correlated features reduce the efficacy of data mining techniques like generalized linear models by increasing the variance in the estimated model coefficients, leading to greater uncertainty in the model outputs. % 
%
Moreover, extra uninformative dimensions make the data exploration stage of the machine learning pipeline very challenging, because there exists no principled way of choosing which of the many dimensions to visualize for an arbitrary dataset. % 
%
\\\\
%
Visualizing data for exploratory purposes is a crucial step in an analysis that motivates the rest of the study, often suggesting a model class or a feature transform. %
%
Classical statistics has some tools to offer, like a matrix-style pairwise scatterplot showing how each pair of feature changes with respect to every other, accessible through the \texttt{pairs} function in the R programming language. % 
%
Such visualizations are instrumental in helping a practitioner choose the right features for a generalized linear model. % 
%
After perhaps nine dimensions, though, these plots become difficult to read and interpret. % 
New visualization tools are needed in the machine learning pipeline to help practitioners deal with the expanding ambition and data scale of the methods. %
%
\\\\
%
This project asks the following research question: can a simple-to-use, automated dimensionality reduction and data-cluster tool improve the workflow of machine learning researchers during the model exploration phase of their analysis? % 
%
The motivation for this study stems from the author's personal experience working with a team of statisticians and mathematicians at an industry mathematical modelling workshop. %
%
The assigned task was to predict the output of an industrial fusion reactor using a subsample of 2 terabytes of time-series data. % 
%
The greatest bottleneck and most time consuming task was to discover an effective visualization of the data that revealed sufficient structure for the team to exploit to build a useful, interpretable predictive model. %
%
\\\\
%
Concretely, the contribution of this research is a high-dimensional data visualization dashboard (referred to as the \textit{Visualization Dashboard} in the remainder). % 
%
The Visualization Dashboard has been developed using the D3.JS and Highcharts libraries in Javascript. %
%
It is hosted on a remote web server and runs locally in a web browser. % 
%
The Visualization Dashboard allows users to easily load data from a locally-hosted file and to generate multiple reduced-dimensionality views on a raw dataset. % 
%
% TODO: maybe a figure here to motivate, as in the presentaiton
%
This views may then be clustered using the k-means centroid-based clustering (Hartigan \& Wong 1979) in order to reveal groups of related points. % 
%
The interface provides sliders for adaptively adjusting the parameters, and a button to re-run each algorithm with the new settings. % 
%
State-of-the-art dimensionality reduction techniques like t-Distributed Stochastic Neighbour Embedding (tSNE) \cite{maaten2008visualizing} require careful tuning of parameters to find useful results. % 
%
The Visualization Dashboard facilitates this tuning through the updatable views and sliders, and provides independent viewing panes for each of the three visualizable reductions (one, two, or three dimensions) and each of the algorithms currently implemented. % 
%
This allows a researcher to experiment and iterate quickly over different visualizations of a subsampled version of the dataset. % 
%
\section{Background and Previous Work}%
%
To the best of the author's knowledge, no previous tool provides an interactive dimensionality reduction and clustering framework with multiple different algorithms in order to aid researchers in data exploration. %
%
Common tools and software frameworks like or RStudio, or Matlab, or iPython require extensive installation and knowledge of the particularities of the languages. %
% TODO: cite Tableau
Tableau, by contrast, is a commercial software tool with a simple interface that can generate many different visualizations. %
%
Tableau is closest in intention to the Visualization Dashboard. %
% http://www.tableau.com/learn/whitepapers/using-r-and-tableau
% https://www.r-bloggers.com/dream-team-combining-tableau-and-r/
For more advanced statistical techniques like dimensionality reduction, though, Tableau relies on user expertise in the R programming language. %
In contrast, the Visualization Dashboard encapsulates the statistical experitise needed to program an advanced algorithm by exposing only sliders and the algorithm output. %
%
\\\\
%
There have been a number of recent research contributions that attempt to acheive a similar level of technical encapsulation for machine learning, and we review these in this section. %
\subsection{Visualizations to Support Machine Learning}
%
EnsembleMatrix \cite{talbot2009ensemblematrix} is a visualization tool designed to assist non-experts in machine learning in the task of combining independently-trained classifiers into the best performing ensemble\footnote{An ensemble classifier aggregates the output of a set of classifiers to make a final classification decision through majority voting, or a the highest probability class out of a weighted combination of the class-membership probabilities contributed by each component classifier.}. %
%
EnsembleMatrix presents a confusion matrix \footnote{A confusion matrix displays the number of classification decisions for each class label in a matrix. The true class label is the row label, and the predicted class label is the column label. A perfect classifier has no entries in the off-diagonal of the matrix.}, coloured as a heat-map, as its central visualization. %
%
Users are given two interface widgets: (1) a polygon and brush tool that determines the weighting of each classifier by the position of the brush; and (2), the ability to partition the data by labels by clicking on the central confusion matrix visualization and apply a custom weighting scheme on that partition. %
%
Control point (1) allows a user to upweight classifiers that perform well on regions of the class space that have been isolated by control point (2). %
%
The combination of (1) and (2) allows a user to recursively identify areas of weakness in the ensemble classifier apply custom weighting schemes to strength the performance. %
%
Each change results in an immediate update of the confusion matrix. %
%
Such adaptive feedback allows fine-grained control over how the classifiers are combined into an ensemble. %
\\\\
A user study of the interface demonstrated that non-expert users were able to achieve state-of-the-art performance on an image classification task \cite{talbot2009ensemblematrix}. %
%
This validates the theory that well-designed visualization tools can give non-expert users access to very effective machine learning tools. %
Both the adaptive updates and interface widget design concept were and inspiration for the Visualization Dashboard. %
%
EnsembleMatrix differs from the Visualization Dashboard because it intervenes in a later stage of the machine learning pipeline, after a group of models has already been constructed. %
%
The Visualization Interface presented in this paper intervens in an earlier stage: determining what model class and what features can be modeled using the dataset. %
%
\\\\
%
ManiMatrix \cite{kapoor2010interactive} is another visualization that intervenes in the machine learning pipeline after a classifier has been trained. %
%
ManiMatrix presents users with a visualization of a cost function
%
\footnote{A cost function is a mathematical tool from decision theory. It formalizes the natural idea that more costly mistakes should be avoided when doing classification (this is not a feature of the base mathematical formulation used in a classifier). %
%
Misclassifying a patient as suffering from the influenza virus when they are really suffering from a rhinovirus infection is not as serious as misclassifying skin cancer as a regular freckle. %
%
A cost function provides the mathematical object that tunes the decisions made by a classifier, allowing a statistician to minimize the risk of the most serious errors.}, a confusion matrix, and the decision boundaries generated by a classifier%
%
\footnote{A decision boundary is a region of the data space in which any point that falls in the region is classified identically. This is commonly visualized by colouring entire decision regions uniformly in a scatterplot of the points.}. %
%
ManiMatrix provides an interface widget that allows a user to change the cost function, represented as a matrix, up or down for each class. %
%
Every change triggers a fast update of the classification algorithm. %
%
This is reflected immediately in changes to the confusion matrix and decision boundary visualizations. %
%
\\\\
%
In a user study, \cite{kapoor2010interactive} found that their system allowed users to quickly and effectively tailor standard algorithms to problems with a complex cost and penalty structure. %
%
Their work demonstrates the gains that information visualization can offer machine learning practitioners by encapsulating algorithmic updates within adaptive displays and intuitive interface components. %
%
Such an interface allows users to avoid the expertise required (or, given the expertise, taxing work of) recalculating the model and re-running, re-visualizing the analysis scripts. %
%
The Visualization Dashboard works to provide the same encapsulation by hiding the unimportant details behind an effective user interface. %
%
ManiMatrix differs from the Visualization Dashboard differs in the same way as EnsembleMatrix: the intervention in the machine learning pipeline by ManiMatrix occurs after the difficult work of deciding on and training a model has been completed.%
%
\subsection{Visualizations to Teach Machine Learning}
%
Another set of research contributions, closer in technical ambitions and software form to the Visualization Dashboard, have been produced in recent years to help teach relatively complex machine learning concepts. \textbf{[TODO: better motivation for this section]}. %
\\\\%
%
The architecture of a trained Convolutional Neural Network (CNN)%
\footnote{Neural networks are represented in ML as node-link diagrams, because their mathematical structure corresponds exactly to a directed graph where each input feature of a datapoint is a root node, and each output features is a leaf nodes.} %
%
is visualized by \cite{harley2015interactive}. %
%
The network is trained to recognize the digit class of a handwritten digit. %
%
The goal of the interface is to enhance a user's understanding of CNNs in general and how they classify inputs. %
%
This is acheived by showing what parts of the network are most active during a classification decision. %
%
Users view a three-dimensional representation of the entire network, with a pane for drawing new digits with the mouse cursor. %
%
Users can traverse the network structure in three dimensions after entering a digit. %
%
Playing with this dynamic interface can help a user see what sorts of errors the network is prone to make, and can give a sense of how the network recognizes a given number. %
%
\\\\
%
Although a valuable learning tool to help users familiarize themselves with CNNs, the Node-Link visualization does not have any immediate applications for tuning or improving CNN models for image recognition. %
%
Moreover, as of writing, practical neural architectures for image recognition are non-trivially more complex than the network visualized in \cite{harley2015interactive}. %
%
Nevertheless, this work is an important first step in the direction of visualizing complex neural architectures. %
%
Its use of three-dimensional  highlight the desire for better visualization methods in machine learning to aid practitioners in understanding and using models.%
%
\\\\
%
A few online teaching tools are more similar in technology and intention to the Visualization Dashboard. %
%
Setosa.io's Explained Visually project demonstrates the Principal Component Analysis (PCA) algorithm \cite{setosaPCA} using both a toy example and a real example. %
%
PCA finds the directions of greatest variance in a dataset, and then projects the data points onto those directions as a way of eliminating redundant or noisy information. %
%
In the Setosa.io interface, users can click and drag points diplayed on a scatterplot to see how changing the location of inputs influences the projection onto 1 or 2 dimensions. %
%
This interactivity can help make the algorithm intuitive to a user, and aid in the interpretation of the results. %
%
\\\\
%
Similarly, the Google Brain research group released a set of visualizations \cite{wattenberg2016how} to help analysts understand how tuning particular parameters\footnote{Sometimes, the tunable components of tSNE are called \textit{hyperparameters} to emphasize their role in determining other parameters at a lower hierarchical level of the algorithm.} affects the output of tSNE \cite{maaten2008visualizing}. %
%
tSNE is the most complex algorithm included in the Visualization Dashboard. %
%
It is one of the most popular%
%
\footnote{In the eight years since its introduction to the time of writing, it has been cited 1973 times.} %
%
and widely used due to its ability to reproduce structure in high dimensions accurately in low dimensions \cite{maaten2008visualizing}. %
%
The tSNE visualizations show toy examples of particular geometrical relationships in high dimensional spaces and shows how tSNE represents them in two dimensions. %
%
They apply the same open-source library as the Visualization Dashboard to generate the tSNE visualizations. %
%
This works shows both the impetus for better visualizations in dimensionality reduction techniques, and also provides an exemplar of a successful set of visualizations. %
%
Moreover, the pedagogical value of \cite{wattenberg2016how} is high, and it has been linked to through the Visualization Dashboard as a source of extra information to help users develop intuition about their data in the reduced dimensional space.
%A recent innovation in t-SNE, one of the algorithms used for the Visualization Dashboard \cite{pezzotti2015approximated}, applies ideas from the field of Progressive Visual Analytics to make intermediate results of t-SNE useful. %
%
%t-SNE is a computationally intensive technique when applied to a large or complex dataset. %
%
%It can require intensive computation to reach optimal embeddings of high dimensional data in low dimensional spaces. %
%
%The work by \cite{pezzotti2015approximated} is an algorithmic and technical innovation that allows parital results from t-SNE to be viewed and adapted in real time to enable data exploration. %
%
%Interestingly, t-SNE is attempting to solve the same problem as the Visualization Dashboard, to help users identify clusters or patterns in data that can be used to generate novel hypothesis. %
%
%The key difference  is that the Visualization Dashboard renders a problem computationlly tractable by subsampling the data randomly in a way that preserves statistical properties, whereas \cite{pezzotti2015approximated} develop approximation methods that are computationally efficient in order to maintain the entire dataset. %
%
%\cite{pezzotti2015approximated} also apply cluster algorithms to help users see developing structures in the low-dimensional representation of the data, and additionally use a Magic Lens visualization to allow users to focus in on a region of interest. %
%
%[FOOTNOTE] a magic lense is a circular area that customizes the opacity of points so that only the points of interest are focused on.%
%

%\textbf{Previous Work in Statistics}

%Statisticians have been dealing with high dimensional datasets since the inception of the field. H

%Scatterplot matricies are their solution

%these work to a point, and then fail

%TODO: bring in the statistics comparison with scatterplot matrix.


\section{Visualization Dashboard}
The Visualization Dashboard is a web application intended for use by analysts in the initial phase of a machine learing project. A portion of the interface is shown in figure [TODO: add figure]. The steps necessary to develop a set of visualizations like this will be described in this section, focusing on what a user is able to acheive with the interface and the design choices that were made.%
%
\\\\
%
\textbf{Technical Overview}
The core functionality of the Visualization Dashboard was written in Javascript, using a number of open source tools to acheive the dimensionality reduction and cluster algorithms. The tools include ml-js for PCA and general linear algebra functionality, tsnejs by Andrej Karpathy for the t-SNE dimensionality reduction algorithm, and mdsjs by  for the MDS algorithm
\cite{tSNEJS}
\cite{highcharts}
\cite{mljs}
\cite{mdsjs}
\cite{papaparse}
\cite{d3js}
%

\textbf{Data Loading} \\
The interface consists of a group of visual widgets as in figure [TODO reference immediately previous figure] stacked vertically. Each widget is a dark region that fills the majority of the browser view window. The design choice of separated but visually similar widgets is intended to reinforce the idea that users are seeing independent and equally valid views of a dataset, differing in terms of the dimensionality reduction algorithms that were used.


The first widget is for loading data and labels into the interface [TODO: data load widget]. Users are presented with two, equal-area dashed boxes with beveled edges inside of a surrounding visual region, darker than the background. The boxes have "X Data: Drop .csv File Here" where X is one of Features, Labels. When a user drags a file over the area, the text changes color to Cyan to offer the possibility of action. Once a user drops a file containiend comma-separated values into the region, the text changes to "Currently loaded: Y" where Y is the name of the file. 

This component of the interface has been designed to be as minimal as possible, with a single dynamic point of change to indicate to a user that an action is possible, or that an action has been completed. Loading data into programming languages like Matlab, Python, or Julia can  be a time-consuming process, because the absolute or relative path of the file on the host operating system must be known beforehand. Smoothing out this process can reduce cognitive load in exploring the data, and increase the efficiency of the overall procedure. The design choice is also meant to encourage users to try loading in different datasets, and potentially different subsampled version of the dataset, which can be easily generated programmatically using command-line tools in linux or generated graphically using Microsoft Excel.

\textbf{1D and 2D Data Reduction} \\
Each dimensionality reduction widget contains three blank subdivisions that are populated with scatterplots based on the reduced-dimensionality data. Scatterplots were chosen for as the central visualization component of the Visualization Dashboard because they are a model-agnostic way of viewing data in 1, 2, or 3 dimensions. This makes a scatterplot representation  The subdivisions are depicted in figure [TODO: figure number]. The three subdivisions are labelled by the dimensionality reduction that is available in each of them. Below the viewing pane are the current value of tunable parameters for the algorithm. Depending on the complexity of the algorithm, more or fewer tunable parameters will be present. For example, with the t-SNE algorithm, the parameters are Perplexity, Learning Rate, and Number of Iterations. Each tunable parameters is controlled by a slider. Moving the sliders dynamically updates the values of the parameters, shown below the lpotting pane.

Once settings for the parameters have been selected, the algorithm can be run using a button labelled "Run the algorithm!". As the mouse cursor moves over this button, it inverts colors so that the dark text becomes light at the same time as the light background becomes dark. Such a dynamic inversion is a common way for web applications to indicate to a user that a region contains a latent possibility for action, and invites a mouse click. A second button in the subdivision is labelled "Cluster with K-means, k = Z" where Z is an integer between 1 and 10. A slider is present above this button, which dynamically changes the value of K. When no dataset is loaded, clicking these buttons had no effect.

\textbf{3D Data Exploration}
Because data in 3D has an additional extension that 2D does not, the interface for 3D exploration is different. Users can click and drag the viewport to rotate around the 3D structure and examine different points of view to look for interesting structures. Because the structures visible in 3D views are highly sensitive to the point of view, users are given the option of saving a particular view of the data in all common image formats. This can be used to compare the views later to decide what statistical model would best separate out the classes or fit a curve for regression.

\textbf{Clustering the Data with K-Means}\\
For the prototype, one clustering algorithm was implemented: k-means. Upon clustering hte data, the different clusters are visualized by a change in point color. The k-means cluster assignments corresponds to assuming that the data was generated from a mixture of Gaussian distributions, each of which has spherical covariance [footnote: explain spherical covariance] and means corresponding to the centers of the colored regions. Although the assumption that the points were generated as random samples from Gaussian distributions may seem unreasonable, it's worth recalling that in terms of Information Theory [TODO: find a MacKay reference], the Gaussian distribution makes the fewest  assumptions of any probability distribution. Moreover, this assumptions allows a convenient clsutering algorithm that is generally very stable, in that it will reach the same clusters on every run with the exception of some points on the fringes of the cluster that could belong to neighbouring groups. The colours themselves will sometimes change, but this is to be expected, because the colours are simply labels and carry no important information other than group membership.

\textbf{Animations and Other Dynamic Components}\\
Animations are also used by the Visualization Dashboard to convey important information. The first dynamic affordance offered by the Visualization Dashboard is the appearance of tooltips over data points that are currently on display. The tooltip appears on mouseover of hte point and disappears when the mouse leaves the region of hte point. The tooltip provides information on the coordinates of the point, and also its row number in the dataset (labelled "Index") in the interface. 

The coordinates are displayed so that the magnitude of difference between nearby points can be found quantitatively. Unlike on a paper plot, it can be difficult to see exactly where a horiztonal or vertical line through a point of interest intersects with the coordinate axes to know the value of a point. On the other hand, displaying every value as a tuple next to the point would be too visually cluttered. Having the actual point value available on mouse-over solves both problems. The index is provided because dimensionality reduction algorithms modify the actual location of the points. If a point was original in six dimensions and is reduced to three, the three remaining dimensions are extremely unlikely to have the same value after the algorithm has run. Hence, the index allows a user to investigate the points back in the original high-dimensional space if desired.

Because some dimensionality reduction algorithms are strongly influenced by their parameters settings, whenever the algorithm is run, an animation occurs in the visual display region. Assuming a dataset has been loaded into the interface, clicking the run button executes the dimensionality reduction algorithm with the current parameters settings. This calculates a new set of positions for the points, creating a new reduced dimensionality dataset. As soon the reduced-dimension dataset is available, all points currently on display inflate in size, and then glide to their new positions.

The purpose of this animation is twofold. The first is to invite a user to play with different settings for the algorithm by stoking their curiousity about the changes that may occur next. This is intended to encourage greater exploration of the possibilities in the dataset. The second reason is to help a user track particular dataset clusters of interest. If a particular group of points appears together under one parameter setting, increasing or decreasing those parameteres might cause the point cluster to either disperse, or contract even more tightly. Determining which behaviour has occurred can be a strong cue to structural features of the dataset in high dimensions that are being reproduced in lower dimensions.

\textbf{Algorithms Available in Prototype}
The current algorithms implement in the prototype are t-Distributed Stochastic Neighbour Embedding (t-SNE), Principal Component Analysis (PCA), and Multidimensional Scaling (MDS). As will be explored in the case study section below, these algorithms each offer different persepctives on the same data, facilitating the exploration of different possible models and questions that could be answered by the dataset. Each dimensional reduction of each of the three algorithms can be generated and controleld independently, meaning that for any run of the Visualization Dashboard on one dataset, 9 independent views can be generated and manipulated.

\textbf{Hyperlinks into the Machine Learning Community}\\
The machine learning community, like many other subdisciplines of comptuer science, is strongly connected online. There are many excellent resources to help users understand dimensionality reduction algorithms [TODO: chain of citations]. Even expert users can benefit from review the effect of different parameters on how the algorithms behave, a fortiori when they are studying a particular dataset in great detail. Hyperlinkns are provided next to the names of the algorithms tha link to such resources. For example, next to the t-SNE algorithm is a hyperlink to a recent Google Brain project on how to tune the t-SNE algorithm when the true dataset has particular structuers. The resource uses the same t-SNE Javascript library as the Visualization Dashboard, but provides numerous toy examples to explore how the hyperparameters expose different structural features of the high dimensional spaces in low dimensions. %
%
\subsection{Case Study: Exploring Latent Structure in a Dataset}
%
TODO: rewrite as exploration.

This section gives an extended example of how the Visualization Dashboard can be used to determine an appropriate model for a dataset that contains a lower-dimensional subspace which captures the important variation point to point. This dataset, called the "Swiss Roll", is a curved structure in 3D. It is often used to study the properties of dimensionality reduction algorithms. Although the structure has curvature, the points that lie on it are related to each other in a relevant way by the location on the "unrolled" roll. The swiss roll is an example of a manifold in geometry, where the relevant distances on the manifold are locally "flat" and described perfectly by Euclidean distances, but the overall space is curved when viewed as a whole. So, we would like to find a lower dimensional representation that is itself flat, corresponding to the best way to describe the relationship among points.

When viewed using the PCA algorithm, it is not clear that the correct structure is found in 2D as in figure [TODO: figure]. Playing around with the 3D curvature, it is clear that PCA has simply reduced teh distances maong points, but kept the rolled structure in space intact. in teh 2D represntation, the third dimension has simply been "flattened", so that points very far away from each other on the manifold are now very close. This is undesirable.

Similarly, MDS finds a different flattening of the space: as if viewing the roll straight on, it has collapsed the width of the roll so that only the spiral shape remains in 2D [TODO: figure]. In 3D, MDS has similarly collapsed the width of the roll but maintained its curved structure, producing a coiled snake-like structure.

t-SNE, on the other hand, appears to be doing what we want. In 2D, it produce a wide and flat structure with some pathological clustering and gaps in the center. nevertheless, it performs generally better than MDS and PCA on this problem. In 3D space, the curvature of the roll is decreased substantially. This shows that t-SNE is learning to ignore the curvature of the overall manifold and naturally represnting the high-dimensional structure using only 2 dimensions. This can inform an analyst that encodign the data usign a neural network with 2 output dimensions could yield features for the dataset that perform best for a classification or regression problem.

%
\section{Information Visualization Theory}%
%
In Information Visualization theory, Ed Chi’s thesis extended a model of the information visualization pipeline \cite{stuart card} that places raw data at the start and a view of the data at the end. Chi’s model is designed to capture the process of analysis that yields multiple visualization of a dataset. Raw data is transformed through four stages: first to an analytical abstraction representing the data as some abstract data structure like a graph, tree, or vector, through to a visualization abstraction like a network diagram, point set, or surface, and finally to a view on the data that displays some salient feature of interest to the analyst. Chi’s model is an extension of Stuart Card’s simpler information visualization pipeline that imagines an analyst proceding through a single path from raw data to final view. Card’s model allows multiple paths, some of which terminate before yeilding a final visualization.

Chi's network model can be extended to model the process by which a machine learningresearcher use the Visualization Dashboard. The extension involves the addition of backlinks to permit cycles through the network, changing the model from a Directed Acylclic Graph to one with cycles. This extended Data Space Model can track the heuristic steps that an analyst takes in discovering relevant structure by tracing the links among the diagrams. This could serve as a map for other researchers looking at similar data to help guide their exploration of the high dimensional space. Since the present does not have strong theoretical goals, further work on developing this idea is left to future projects. [!!!!!!!!!!!!!!!!!11TODO: add in figure]

Another concept in information visualization useful for modeling this project can be found in the cost structure of sensemaking \cite{russel and card}. Sensemaking describes how an analyst searches through the space of possible methods for answering a question, given some visualization or other information. The ideal sensemaking system provides a richer space of models to search through without taking too much additional time on the part of the analyst. The goal of the Visualization Dashboard is to minimize the time cost of running complex analyses using deimnsionality reduction algorithms. Software libraries in popular scientific computing languages like Matlab and Python can acheive greater control over the algorithms and datasets, but at the cost of aditional experitse required. Even loading data into the graphical user interface can involve many small details that take time away from the exploration value. In this senes, the Visualization Dashboard reduces the time cost of dataset exploration down to the minimum while still allowing the rich high-dimensional visualization within it to guide an anlayst to the best model.



\subsection{Evaluation}
A formative study was conducted in the late stages of development, after a working prototype was completed. The study was conducted with one participant, who is a graduate student in the machine learning group at the University of Toronto. The format of the study was an unstructured interview.

The participant was told that the interface was useful for statisticians and machine learning practicioners to understand high dimensional data. The participant was then shown the interface, and asked to explore its functions and describe what it was intended for. The participant was able to determine that a data file could be loaded into the interface, and that the interface would react to a data file being loaded by showing different visualizations of the points. This suggests that the basic functionality of the interface is evident from the design, and that no major flaws exist in the design concept.

The participant was then given access to a data file and asked to use the interface to explore it. After loading the file in the interface, he successfully generated a number of different visualizations. After the participant discovered the three-dimensional visualization which allows rotation around the point cloud, he attempted to click and dragging the two and one dimensional visualizations in order to elicit a panning response from the view. He remarked on this missing functionality and noted that panning and zooming in the 1 and 2 dimensional representations would be a valuable addition.

A second piece of feedback from the participant was that there should be the abililty to view the raw data itself within the interface in a tabular, editable format. In order to understand the clustering procedure, he noted, it would be useful to add in synthetic data points and have them appear dynamically within the reduced dimensional representations.

These results are suggestive that the interface is effective in allowing exploration and study of complex datasets. The kinds of visualizations and play allowed by the current interface could be fruitfully extended in future enhancements to the interface. These enhancements can be built into the existing framework in a straightforward manner, now that the core functionality has been developed.

\subsection{Discussion and Conclusion}
(Hold until tomorrow after edits)


\subsection{First Page Copyright Notice}
This template include a sample ACM copyright notice at the bottom of
page 1, column 1.  Upon acceptance, you will be provided with the
appropriate copyright statement and unique DOI string for publication.
Accepted papers will be distributed in the conference
publications. They will also be placed in the ACM Digital Library,
where they will remain accessible to thousands of researchers and
practitioners worldwide. See
\url{http://acm.org/publications/policies/copyright_policy} for the
ACM's copyright and permissions policy.



\section{Conclusion}

It is important that you write for the SIGCHI audience. Please read
previous years' proceedings to understand the writing style and
conventions that successful authors have used. It is particularly
important that you state clearly what you have done, not merely what
you plan to do, and explain how your work is different from previously
published work, i.e., the unique contribution that your work makes to
the field. Please consider what the reader will learn from your
submission, and how they will find your work useful. If you write with
these questions in mind, your work is more likely to be successful,
both in being accepted into the conference, and in influencing the
work of our field.

% References must be the same font size as other body text.
\bibliographystyle{SIGCHI-Reference-Format}
\bibliography{sample}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
